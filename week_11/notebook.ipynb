{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29c7f4d-134f-42e5-bf07-6165a48e9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys, json\n",
    "os.environ['HF_HOME'] = '/home/jovyan/shares/SR003.nfs2/.cache/huggingface'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "610aad3b-3b38-415d-88e6-3e9eb88e1d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import yaml, json\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f5c920-0b37-4c6d-90bf-bbbdbb159fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7fd38c-fda6-41fd-bbf5-ae6fa453c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6a2ed-f457-4ca0-b782-93ca6d9625c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c4edc-4de3-452e-9df2-73a51ac1ac0c",
   "metadata": {},
   "source": [
    "### mPLUG-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7816f9a0-f8cb-4607-8d48-5dd4c3cb26bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tifa'...\n",
      "remote: Enumerating objects: 271, done.\u001b[K\n",
      "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 271 (delta 73), reused 67 (delta 67), pack-reused 189 (from 1)\u001b[K\n",
      "Receiving objects: 100% (271/271), 5.97 MiB | 8.33 MiB/s, done.\n",
      "Resolving deltas: 100% (143/143), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Yushi-Hu/tifa.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00420616-4e2c-4038-b334-d1543be987d3",
   "metadata": {},
   "source": [
    "Будем работать с задачей zeroshot VQA. Загружаем large версию модели mPLUG. Воспользуемся реализацией VQA через метчинг логитов с вариантами ответа из бенчмарка TIFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b204588c-20f2-4042-beec-68406600e280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/crave/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/home/jovyan/.mlspace/envs/crave/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-31 11:50:30,579] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/crave/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jovyan/.mlspace/envs/crave/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mplug-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 11:50:38,122 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/jovyan/.cache/modelscope/hub/models/damo/mplug_visual-question-answering_coco_large_en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 11:50:41,174 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2025-05-31 11:50:41,517 - modelscope - INFO - initiate model from /home/jovyan/.cache/modelscope/hub/models/damo/mplug_visual-question-answering_coco_large_en\n",
      "2025-05-31 11:50:41,518 - modelscope - INFO - initiate model from location /home/jovyan/.cache/modelscope/hub/models/damo/mplug_visual-question-answering_coco_large_en.\n",
      "2025-05-31 11:50:41,520 - modelscope - INFO - initialize model from /home/jovyan/.cache/modelscope/hub/models/damo/mplug_visual-question-answering_coco_large_en\n",
      "/home/jovyan/.mlspace/envs/crave/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1706: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/crave/lib/python3.10/site-packages/modelscope/models/multi_modal/mplug/modeling_mplug.py:1873: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/jovyan/.cache/modelscope/hub/models/damo/mplug_visual-question-answering_coco_large_en/pytorch_model.bin\n",
      "<All keys matched successfully>\n",
      "Finish loading mplug-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/crave/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SBERT on GPU\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('tifa')\n",
    "from tifascore import VQAModel\n",
    "model = VQAModel(\"mplug-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65a49c-33c0-40b5-8275-3a8a118ae557",
   "metadata": {},
   "source": [
    "### LLaVA 1.5 HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff498331-be3e-4154-bc00-753d6866066f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1abd1d8-c26b-46f8-85c0-1a58e3d40264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text: str) -> list:\n",
    "    conversation = [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "              {\n",
    "                  \"type\": \"text\", \"text\": \"Your task is to answer in the format of a visual question answer.\"\\\n",
    "                                       \"If the question requires a 'yes' or 'no' answer, the answer should contain only 'yes' or 'no'.\"\\\n",
    "                                       \"If the question requires a choice of answer options ('A' or 'B'), the answer should contain only the letter 'A' or 'B'.\"\n",
    "              }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": text},\n",
    "              {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "945dc2b5-b9f3-4b6f-acbe-8f791a7a3180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.60it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-1.5-13b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to('cuda')\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535ec2c-bfce-4eef-9608-49a507a979ab",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "222bd431-2b71-49f4-be76-75a1ab2caaa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# датасет VQA\n",
    "dataset = load_dataset(\"BaiqiL/NaturalBench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af89f6c2-8f71-478c-8ee8-91bf9bb07d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43e073a5-9457-477b-b009-945d0a358498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def evaluate_mplug(dataset, model):\n",
    "    result = 0\n",
    "    results_all = []\n",
    "    for d in tqdm(dataset):\n",
    "        sample_res = 0\n",
    "        question_0 = d['Question_0']\n",
    "        question_1 = d['Question_1']\n",
    "        image_0 = d['Image_0']\n",
    "        image_1 = d['Image_1']\n",
    "        image_0.save('im0.png')\n",
    "        image_1.save('im1.png')\n",
    "        ans_i0_q0 = d['Image_0_Question_0']\n",
    "        false_i0_q0 = 'yes' if ans_i0_q0 == 'no' else 'no'\n",
    "        false_i0_q0 = 'A' if ans_i0_q0 == 'B' else 'B'\n",
    "        \n",
    "        ans_i1_q0 = d['Image_1_Question_0']\n",
    "        false_i1_q0 = 'yes' if ans_i1_q0 == 'no' else 'no'\n",
    "        false_i1_q0 = 'A' if ans_i1_q0 == 'B' else 'B'\n",
    "        \n",
    "        ans_i0_q1 = d['Image_0_Question_1']\n",
    "        false_i0_q1 = 'yes' if ans_i0_q1 == 'no' else 'no'\n",
    "        false_i0_q1 = 'A' if ans_i0_q1 == 'B' else 'B'\n",
    "        \n",
    "        ans_i1_q1 = d['Image_1_Question_1']\n",
    "        false_i1_q1 = 'yes' if ans_i1_q1 == 'no' else 'no'\n",
    "        false_i1_q1 = 'A' if ans_i1_q1 == 'B' else 'B'\n",
    "        outp_i0_q0 = model.multiple_choice_vqa(\"im0.png\", question_0, choices=[ans_i0_q0, false_i0_q0])['multiple_choice_answer']\n",
    "        outp_i0_q1 = model.multiple_choice_vqa(\"im0.png\", question_1, choices=[ans_i0_q1, false_i0_q1])['multiple_choice_answer']\n",
    "        outp_i1_q0 = model.multiple_choice_vqa(\"im1.png\", question_0, choices=[ans_i1_q0, false_i1_q0])['multiple_choice_answer']\n",
    "        outp_i1_q1 = model.multiple_choice_vqa(\"im1.png\", question_1, choices=[ans_i1_q1, false_i1_q1])['multiple_choice_answer']\n",
    "        if outp_i0_q0 == ans_i0_q0 and outp_i1_q1 == ans_i1_q1 and outp_i0_q1 == ans_i0_q1 and outp_i1_q1 == ans_i1_q1:\n",
    "            sample_res = 1\n",
    "            result += 1\n",
    "        results_all.append(sample_res)\n",
    "    return result / len(dataset), results_all     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c66a2eb-62c7-4aa2-9671-8fc0740f5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def LLaVA_inference(model, text, image):\n",
    "    prompt = processor.apply_chat_template(create_prompt(text), add_generation_prompt=True)\n",
    "    inputs = processor(images=image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "    output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    output = processor.decode(output[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    output = output.strip().lower()\n",
    "    output = re.split(r'[;:?!,\\. ]', output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8eef08e-d9a1-4562-8ff3-ed9865838ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LLaVA(dataset, model):\n",
    "    result = 0\n",
    "    results_all = []\n",
    "    for d in tqdm(dataset):\n",
    "        sample_res = 0\n",
    "        question_0 = d['Question_0']\n",
    "        question_1 = d['Question_1']\n",
    "        image_0 = d['Image_0']\n",
    "        image_1 = d['Image_1']\n",
    "        ans_i0_q0 = d['Image_0_Question_0']\n",
    "        ans_i1_q0 = d['Image_1_Question_0']\n",
    "        ans_i0_q1 = d['Image_0_Question_1']\n",
    "        ans_i1_q1 = d['Image_1_Question_1']\n",
    "        \n",
    "        outp_i0_q0 = LLaVA_inference(model, question_0, image_0)\n",
    "        outp_i1_q0 = LLaVA_inference(model, question_0, image_1)\n",
    "        outp_i0_q1 = LLaVA_inference(model, question_1, image_0)\n",
    "        outp_i1_q1 = LLaVA_inference(model, question_1, image_1)\n",
    "        if ans_i0_q0.lower() in outp_i0_q0 and ans_i1_q1.lower() in outp_i1_q1 \\\n",
    "            and ans_i0_q1.lower() in outp_i0_q1 and ans_i1_q1.lower() in outp_i1_q1:\n",
    "            sample_res = 1\n",
    "            result += 1\n",
    "            # print(result)\n",
    "        results_all.append(sample_res)\n",
    "    return result / len(dataset), results_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3799795-bea6-4642-bc79-859f88cb6f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_mplug, results_mplug_all = evaluate_mplug(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4351b4d-8a91-4fed-942e-20133bc65a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(result_mplug, 'result_mplug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d7357-3d62-4aa9-90a7-c42ea5c377e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_LLaVA, results_LLaVA_all = evaluate_LLaVA(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3820c13-3118-460a-8a94-f02105b7ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mplug = pd.read_pickle('result_mplug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8a75725-d063-4b1b-b6cd-54f4c1014ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mPLUG-large results:  0.77\n"
     ]
    }
   ],
   "source": [
    "print('mPLUG-large results: ', round(result_mplug, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48d0bc14-5c41-41f4-b57e-af3dbf4e3a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA 1.5 (Vicuna-13b) results:  0.29\n"
     ]
    }
   ],
   "source": [
    "print('LLaVA 1.5 (Vicuna-13b) results: ', round(result_LLaVA, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d791cb8-98c3-4698-af89-1a2fe6dc7580",
   "metadata": {},
   "source": [
    "#### Вывод:\n",
    "- Под задачу VQA лучше обучена модель mPLUG. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-crave]",
   "language": "python",
   "name": "conda-env-.mlspace-crave-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
